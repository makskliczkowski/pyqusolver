"""
NQS Dataset Management
======================

This module provides classes for managing datasets generated by NQS and ED simulations.
It handles computation of observables, formatting for storage, and HDF5 I/O.

-------------------------------------------------------------------------------
File        : NQS/src/nqs_dataset.py
Author      : Maksymilian Kliczkowski
-------------------------------------------------------------------------------
"""

import  time
import  numpy as np
from    pathlib     import Path
from    dataclasses import dataclass, field
from    typing      import Dict, Any, Union, List, Tuple, Optional

try:
    from QES.general_python.common.hdf5man      import HDF5Manager, Directories
    from QES.general_python.common.flog         import get_global_logger
except ImportError:
    raise ImportError("Required modules not found. Ensure that QES.general_python.common is in the PYTHONPATH.")

logger = get_global_logger()

@dataclass
class CommonDataset:
    '''
    Base class for quantum simulation datasets.
    '''
    num_states      : int           = 0
    hilbert_size    : int           = 0
    losses          : Optional[np.ndarray] = None

    lattice_type    : str           = ""
    model_type      : str           = ""
    model_params    : dict          = field(default_factory=dict)

    # list of correlator types to compute (e.g., ['xx', 'zz'])
    correlators     : list          = field(default_factory=lambda: ['xx', 'yy', 'zz'])
    correlations    : dict          = field(default_factory=dict)
    # magnetization components (e.g., {'x': [...], 'y': [...], 'z': [...]})
    magnetization   : dict          = field(default_factory=dict)
    # others like plaquette values, stored as dict or array depending on the model
    plaquettes      : Any           = None
    
    data_output     : dict          = field(default_factory=dict)

    def _prepare_data_dict(self) -> Dict[str, Any]:
        """Prepares the dictionary for HDF5 saving."""
        data_dict   = {
            'hilbert_size'          : self.hilbert_size,
            'num_states'            : self.num_states,
        }
        if self.losses is not None:
            data_dict['energy_values']  = self.losses
        if self.model_params:
            data_dict['model_params']   = self.model_params
        return data_dict

    def save(self, output_dir: Union[str, Path], filename: str):
        """Saves the dataset to an HDF5 file."""
        directory   = Directories(str(output_dir))
        directory.mkdir(exist_ok=True)
        data        = self._prepare_data_dict()
        try:
            HDF5Manager.save_hdf5(str(directory.path), filename, data, override=True)
            logger.info(f"Saved dataset to {directory.path / filename}", color='green', lvl=1)
        except Exception as e:
            logger.error(f"Failed to save dataset: {e}")

@dataclass
class EDDataset(CommonDataset):
    '''
    Dataset for Exact Diagonalization results.
    '''
    def _prepare_data_dict(self) -> Dict[str, Any]:
        data = super()._prepare_data_dict()
        if self.correlations:
            for op_key, arr in self.correlations.items():
                data[f'correlations/{op_key}/expectation'] = arr
                
        if self.plaquettes:
            # handle both dict and array formats
            if isinstance(self.plaquettes, dict):
                p_vals = []
                for k, v in self.plaquettes.items(): p_vals.append(v)
                data['plaquettes/expectation'] = np.array(p_vals)
            else:
                data['plaquettes/expectation'] = self.plaquettes
        
        if self.magnetization:
            for comp, vals in self.magnetization.items():
                data[f'magnetization/{comp}/expectation'] = vals
        
        return data

    def get_operators(self, hamil: Any, nstates_to_store: int = 1) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
        """Compute correlations and magnetization from Hamiltonian eigenvectors."""
        
        if not hasattr(hamil, 'correlators'):
            return {}, {}
            
        values          = hamil.correlators(
            compute                 =   True, 
            nstates_to_store        =   nstates_to_store, 
        )
        
        magnetization   = values.get('magnetization', {})
        correlations    = {}
        ns              = hamil.lattice.ns
        
        for op_key in self.correlators:
            if op_key in values:
                raw_arr     = values[op_key]
                full_corr   = raw_arr.copy()
                # Fill lower triangle for hermiticity
                for i in range(ns):
                    for j in range(i + 1, ns):
                        full_corr[j, i, :] = full_corr[i, j, :].conj()
                correlations[op_key] = full_corr
        return correlations, magnetization

    def get_plaquette(self, hamil: Any, nstates_to_store: int = 1):
        """Compute plaquette expectation values."""
        if hamil.eig_vec is None: return None
        evecs           = hamil.eig_vec[:, :nstates_to_store]
        plaquettes      = hamil.lattice.calculate_plaquettes()
        
        # Simplified Wilson loop computation logic
        # Expectation: <Psi| W_p |Psi>
        # W_p = product of sigma along the plaquette
        # Implementation depends on model specifics (e.g. bond colors)
        # This is a placeholder for the general pattern
        return None 

    def compute(self, hamil: Any, nstates_to_store: int = 1):
        """Compute ED Observables"""
        self.losses                             = hamil.eig_val  
        self.correlations, self.magnetization   = self.get_operators(hamil, nstates_to_store)
        self.plaquettes                         = self.get_plaquette(hamil, nstates_to_store)
        self.data_output                        = self._prepare_data_dict()
        return self.correlations, self.magnetization, self.plaquettes, self.data_output

@dataclass
class NQSDataset(CommonDataset):
    '''
    Dataset for Neural Quantum States results.
    '''
    history         : Any           = None
    net_type        : str           = ""
    sampler_params  : dict          = field(default_factory=dict)
    
    def _prepare_data_dict(self) -> Dict[str, Any]:
        data = super()._prepare_data_dict()
        if self.correlations:
            for op_key, site_dict in self.correlations.items():
                try:
                    # Expecting site_dict to be keyed by (i, j, ...)
                    max_idx = 0
                    for k in site_dict.keys():
                        max_idx = max(max_idx, k[0], k[1])
                    max_idx += 1
                    
                    mean_arr = np.zeros((max_idx, max_idx), dtype=complex)
                    err_arr  = np.zeros((max_idx, max_idx), dtype=float)
                    
                    for (i, j, *rest), stat in site_dict.items():
                        mean_arr[i, j] = stat['mean']
                        err_arr[i, j]  = stat.get('err', stat.get('error_of_mean', 0.0))
                        
                    data[f'correlations/{op_key}/mean'] = mean_arr
                    data[f'correlations/{op_key}/err']  = err_arr
                except Exception as e:
                    logger.warning(f"Could not format NQS correlations for {op_key}: {e}")

        if self.plaquettes and isinstance(self.plaquettes, dict):
            p_means, p_errs, p_sites = [], [], []
            for k, v in self.plaquettes.items():
                p_means.append(v['mean'])
                p_errs.append(v.get('err', 0.0))
                p_sites.append(v.get('sites', []))
            data['plaquettes/mean']  = np.array(p_means)
            data['plaquettes/err']   = np.array(p_errs)
            data['plaquettes/sites'] = np.array(p_sites)
        
        if self.magnetization:
            for comp, comp_data in self.magnetization.items():
                data[f'magnetization/{comp}/mean'] = comp_data['mean']
                data[f'magnetization/{comp}/err']  = comp_data['err']
        
        if self.history is not None:
            if hasattr(self.history, 'to_dict'):
                data.update(self.history.to_dict())
            else:
                data['training_history'] = self.history
        return data
    
    def get_operators(self, nqs: Any, num_samples: int = 1000) -> Tuple[Dict[str, Dict], Dict[str, Dict]]:
        """Compute correlations and magnetization via MC sampling of the NQS."""
        lattice = nqs.model.lattice
        N       = lattice.ns
        
        # Sample once for all observables
        (_, _), (states, log_psi), probs = nqs.sample(num_samples=num_samples)
        
        # Optimized correlation computation
        values = {op: {} for op in self.correlators}
        for name in self.correlators:
            corr_kernels = nqs.model.operators.correlators(correlators=[name], type_acting='correlation', compute=False)
            kernel_jax = corr_kernels[name]['i,j'].jax
            
            for i in range(N):
                for j in range(i, N):
                    res = nqs.compute_observable(states=states, ansatze=log_psi, functions=kernel_jax, probabilities=probs, return_stats=True, args=(i, j))
                    stats = {'mean': res.mean, 'err': res.error_of_mean}
                    values[name][(i, j, 0)] = stats
                    # Symmetrize
                    values[name][(j, i, 0)] = {'mean': np.conj(stats['mean']), 'err': stats['err']}
        
        # Magnetization
        magnetization = {c: {'mean': np.zeros(N, dtype=complex), 'err': np.zeros(N)} for c in ['x', 'y', 'z']}
        for comp in ['x', 'y', 'z']:
            op_jax = getattr(nqs.model.operators, f'sig_{comp}')(lattice=lattice, type_act='local').jax
            for i in range(N):
                res = nqs.compute_observable(states=states, ansatze=log_psi, functions=op_jax, probabilities=probs, return_stats=True, args=(i,))
                magnetization[comp]['mean'][i] = res.mean
                magnetization[comp]['err'][i]  = res.error_of_mean
                
        return values, magnetization

    def compute(self, nqs: Any, num_samples: int = 1000, history: Any = None):
        """Run the full observation suite."""
        self.history = history
        self.correlations, self.magnetization = self.get_operators(nqs, num_samples)
        # Plaquette logic would go here
        self.data_output = self._prepare_data_dict()
        return self.data_output
